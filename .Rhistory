knitr::opts_chunk$set(echo = TRUE)
newdoor <- function(door.number){
door <- list(number = door.number)
class(door) <- "door"
return(door)
}
door1 <- newdoor(1)
door2 <- newdoor(2)
door3 <- newdoor(3)
door1
door1$number
#Make a method for playgame
playgame <- function(door) {
UseMethod("playgame")
}
playgame.door <- function(door){
x = sample(1:3,1)
if(x==door$number){"Congratulations! You have won a brand new car!"}
else{"Awww too bad! You chose the wrong door. Better luck next time!"}
}
playgame.door(door1)
playgame.door(door1)
playgame.door(door1)
playgame.door(door1)
playgame.door(door1)
playgame.door(door1)
playgame.door(door1)
playgame.door(door1)
playgame.door(door1)
playgame.door(door1)
playgame.door(door1)
knitr::opts_chunk$set(echo = TRUE)
# Change eval=FALSE in the code block. Install packages as appropriate.
install.packages("fivethirtyeight")
library(fivethirtyeight)
library(tidyverse)
# URL to the data that you've used.
url <- 'https://jmontgomery.github.io/PDS/Datasets/president_primary_polls_feb2020.csv'
polls <- read_csv(url)
Endorsements <- endorsements_2020 # from the fiverthirtyeight package
#Change variable name, using rename function and store it in our original df
Endorsements <- Endorsements %>% rename(candidate_name = endorsee)
#Change to a tibble and check the class
Endorsements <- Endorsements %>% as_tibble()
class(Endorsements)
#Filter to the candidates with an in command of a vector of the candidates of interest
polls <- polls %>%
filter(polls$candidate_name %in% c("Amy Klobuchar","Bernard Sanders", "Elizabeth Warren", "Joseph R. Biden Jr.", "Michael Bloomberg", "Pete Buttigieg")) %>%
select(candidate_name, sample_size, start_date, party, pct)
#build a table based on the candidate name to confirm only those 6 remain in our new object
table(polls$candidate_name)
#use summary to see that there are only 5 remaining variables as told to do
summary(polls)
#Check spelling of names in our Endorsements dataset
table(Endorsements$candidate_name)
#Here we can tell Amy Klobuchar, Elizabeth Warren, and Pete Buttigieg are spelled the same. Biden is "Joe Biden" here, while "Joseph R. Biden Jr." in our polls set, Sanders is "Bernie Sanders" here instead of "Bernard Sanders", and Bloomberg does not seem to exist in the endorsements set at all.
#Now we need to change these using recode
Endorsements <- Endorsements %>%
mutate(candidate_name=recode(candidate_name, "Bernie Sanders" = "Bernard Sanders", "Joe Biden" = "Joseph R. Biden Jr."))
#Combine them into a dataset using an inner join by candidate name, then verify only the 5 expected remain
pe_combined <- polls_t6 %>%
inner_join(Endorsements, by="candidate_name")
#Change variable name, using rename function and store it in our original df
Endorsements <- Endorsements %>% rename(candidate_name = endorsee)
#Change variable name, using rename function and store it in our original df
Endorsements <- Endorsements %>% rename(candidate_name = endorsee)
#Change variable name, using rename function and store it in our original df
Endorsements <- Endorsements %>% rename(candidate_name = endorser)
#Change variable name, using rename function and store it in our original df
Endorsements <- Endorsements %>% rename(candidate_name = endorser)
# Change eval=FALSE in the code block. Install packages as appropriate.
install.packages("fivethirtyeight")
library(fivethirtyeight)
library(tidyverse)
# URL to the data that you've used.
url <- 'https://jmontgomery.github.io/PDS/Datasets/president_primary_polls_feb2020.csv'
polls <- read_csv(url)
Endorsements <- endorsements_2020 # from the fiverthirtyeight package
install.packages("fivethirtyeight")
#Change variable name, using rename function and store it in our original df
Endorsements <- Endorsements %>% rename(candidate_name = endorsee)
#Change to a tibble and check the class
Endorsements <- Endorsements %>% as_tibble()
class(Endorsements)
#Filter to the candidates with an in command of a vector of the candidates of interest
polls <- polls %>%
filter(polls$candidate_name %in% c("Amy Klobuchar","Bernard Sanders", "Elizabeth Warren", "Joseph R. Biden Jr.", "Michael Bloomberg", "Pete Buttigieg")) %>%
select(candidate_name, sample_size, start_date, party, pct)
#build a table based on the candidate name to confirm only those 6 remain in our new object
table(polls$candidate_name)
#use summary to see that there are only 5 remaining variables as told to do
summary(polls)
#Check spelling of names in our Endorsements dataset
table(Endorsements$candidate_name)
#Now we need to change these using recode
Endorsements <- Endorsements %>%
mutate(candidate_name=recode(candidate_name, "Bernie Sanders" = "Bernard Sanders", "Joe Biden" = "Joseph R. Biden Jr."))
#Combine them into a dataset using an inner join by candidate name, then verify only the 5 expected remain
pe_combined <- polls %>%
inner_join(Endorsements, by="candidate_name")
table(pe_combined$candidate_name)
#Count the number of endorsements for each of the five
wide_pe_combined <- pe_combined %>%
group_by(candidate_name) %>%
summarize(length(points))
p <- ggplot(data = wide_pe_combined) + geom_col(aes(candidate_name,`length(points)`))
p + theme_dark()
p + labs(y = "Number of Endorsements", x = "Candidate", title = "2020 Democratic Presidential Candidate Endorsements")
ggsave("candidate_endorsements_2020.pdf",
plot = last_plot())
# Change eval=FALSE in the code block. Install packages as appropriate.
library(tidyverse)
#install.packages('tm')
library(tm)
#install.packages('lubridate')
library(lubridate)
#install.packages('wordcloud')
library(wordcloud)
trump_tweets_url <- 'https://politicaldatascience.com/PDS/Datasets/trump_tweets.csv'
tweets <- read_csv(trump_tweets_url)
created_split <- tweets %>% mutate(date_created = str_split(created_at, pattern = " "))
created_split
created_split$date_created
time <- as.data.frame(created_split$date_created)
time <- as.data.frame(created_split$date_created)
time
time
time <- t(as.data.frame(created_split$date_created))
time <- t(as.data.frame(created_split$date_created))
time
colnames(time) <- c("date","time")
time
table(time)
tweets <- c(tweets,time)
tweets$date
# Change eval=FALSE in the code block. Install packages as appropriate.
library(tidyverse)
#install.packages('tm')
library(tm)
#install.packages('lubridate')
library(lubridate)
#install.packages('wordcloud')
library(wordcloud)
trump_tweets_url <- 'https://politicaldatascience.com/PDS/Datasets/trump_tweets.csv'
tweets <- read_csv(trump_tweets_url)
created_split <- tweets %>% mutate(date_created = str_split(created_at, pattern = " "))
time <- t(as.data.frame(created_split$date_created))
time <- t(as.data.frame(created_split$date_created))
colnames(time) <- c("date","time")
tweets <- cbind(tweets,time)
tweets
min(tweets$date)
max(tweets$date)
class(tweets$date)
tweets$date <- as.Date(tweets$date)
table(tweets$date)
tweets$date <- as.Date(tweets$date)
install.packages("anytime")
library(anytime)
tweets$date <- anydate(tweets$date)
min(tweets$date)
max(tweets$date)
#oldest tweet day
min(tweets$date)
#newest tweet day
max(tweets$date)
#Now to subset only to tweets
tweets_only <- tweets %>% filter(is_retweet = FALSE)
table(tweets_only$is_retweet)
#Now to subset only to tweets
tweets_only <- tweets %>% filter(is_retweet == FALSE)
table(tweets_only$is_retweet)
table(tweets$is_retweet)
#get the most popular
most_liked <- tweets_only %>%
slice_max(favorite_count, n=5)
most_liked
most_liked
most_retweeted <- tweets_only %>%
slice_max(retweet_count, n=5)
most_retweeted
#Now for the corpus
Corpus <- VCorpus(VectorSource(tweets_only$text))
writeLines(head(stwrap(Corpus[[1]]),10))
Corpus
Corpus[1]
Corpus$1
Corpus$1
Corpus$[1]
Corpus[1]
Corpus[1]$meta
Corpus[1]$author
Corpus[1]$meta$author
library(knitr)
library(kableExtra)
install.packages("kableExtra")
library(kableExtra)
writeLines(head(stwrap(Corpus[[1]]),10))
writeLines(head(strwrap(Corpus[[1]]),10))
#check that it is working
writeLines(head(strwrap(Corpus[[2]]),10))
#check that it is working
writeLines(head(strwrap(Corpus[[3]]),10))
#check that it is working
writeLines(head(strwrap(Corpus[[90]]),10))
#check that it is working
writeLines(head(strwrap(Corpus[[199]]),10))
#check that it is working
writeLines(head(strwrap(Corpus[[1990]]),10))
#check that it is working
writeLines(head(strwrap(Corpus[[420]]),10))
#Now to clean the data
Corpus <- Corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(stripWhitespace) %>%
tm_map(removeWords, stopwords("english"))
#next I'll remove punctuation, numbers, and urls
Corpus <- Corpus %>%
tm_map(removepattern, "?(f|ht)(tp)(s?)(://)(.*)(.|/])(.*)") %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers)
#next I'll remove punctuation and numbers
Corpus <- Corpus %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers)
#Test
writeLines(head(strwrap(Corpus[[420]]),10))
wc <- wordcloud(Corpus, min.freq = 3,
random.order = T,
random.color = T,
max.words = 50)
wc <- wordcloud(Corpus, min.freq = 3,
random.order = T,
random.color = T,
max.words = 50,
random.color = T)
wc <- wordcloud(Corpus, min.freq = 3,
random.order = T,
random.color = T,
max.words = 50,
colors = sample(colors()))
vignette("tm")
#Document Term matrix
dtm <- DocumentTermMatrix(Corpus, control = list(weighting = weightTfIdf))
dtm
inspect(dtm)
#Get the minimum scores:
dtm <- tidy(dtm)
library(broom)
#Get the minimum scores:
dtm <- tidy(dtm)
install.packages("tidytext")
library(tidytext)
#Get the minimum scores:
dtm <- tidy(dtm)
View(dtm)
knitr::opts_chunk$set(echo = TRUE)
# Change eval=FALSE in the code block. Install packages as appropriate.
library(tidyverse)
#install.packages('tm')
library(tm)
#install.packages('lubridate')
library(lubridate)
#install.packages('wordcloud')
library(wordcloud)
library(knitr)
library(broom)
library(tidytext)
trump_tweets_url <- 'https://politicaldatascience.com/PDS/Datasets/trump_tweets.csv'
tweets <- read_csv(trump_tweets_url)
#library a package necessary for date conversion
library(anytime)
#split the data using mutate and str_split
created_split <- tweets %>% mutate(date_created = str_split(created_at, pattern = " "))
#convert it to the correct col/row orientation
time <- t(as.data.frame(created_split$date_created))
#name columns
colnames(time) <- c("date","time")
#combine into our original dataset
tweets <- cbind(tweets,time)
#convert to date format for finding the oldest and newest
tweets$date <- anydate(tweets$date)
#oldest tweet day
min(tweets$date)
#newest tweet day
max(tweets$date)
#So the range of dates is 1/1/2014 to 2/14/2020
#Now to subset only to tweets
tweets_only <- tweets %>% filter(is_retweet == FALSE)
#verify our result
table(tweets_only$is_retweet)
#get the most popular
most_liked <- tweets_only %>%
slice_max(favorite_count, n=5)
most_liked$text
#Get the most retweeted
most_retweeted <- tweets_only %>%
slice_max(retweet_count, n=5)
most_retweeted$text
#Now for the corpus
#Create the corpus
Corpus <- VCorpus(VectorSource(tweets_only$text))
#check that it is working
writeLines(head(strwrap(Corpus[[420]]),10))
#Now to clean the tweets; I'm splitting it for processing power, but keeping some together for efficiency
#first I'll make transform everything to lower case, remove whitespace, and stopwords
Corpus <- Corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(stripWhitespace) %>%
tm_map(removeWords, stopwords("english"))
#next I'll remove punctuation and numbers
Corpus <- Corpus %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers)
#Test
writeLines(head(strwrap(Corpus[[420]]),10))
#Make the word cloud with random color and order
wc <- wordcloud(Corpus, min.freq = 3,
random.order = T,
random.color = T,
max.words = 50,
colors = sample(colors()))
#Document Term matrix
dtm <- DocumentTermMatrix(Corpus, control = list(weighting = weightTfIdf))
#Get the top 50:
#
dtm <- tidy(dtm)
top50_terms <- dtm %>%
group_by(term) %>%
filter(tf_idf == max(tf_idf)) %>%
ungroup() %>%
slice_max(tf_idf, n=50)
#calculate the scores
dtm_tfidf <- bind_tf_idf(dtm_tf_df, term = term, document = document, n = count)
#calculate the scores
dtm_tfidf <- bind_tf_idf(dtm, term = term, document = document, n = count)
top50_terms <- dtm_tf_idf %>%
group_by(term) %>%
filter(tf_idf == max(tf_idf)) %>%
ungroup() %>%
slice_max(tf_idf, n=50)
top50_terms <- dtm_tfidf %>%
group_by(term) %>%
filter(tf_idf == max(tf_idf)) %>%
ungroup() %>%
slice_max(tf_idf, n=50)
head(top50_terms)
#Document Term matrix
dtm <- DocumentTermMatrix(Corpus, control = list(weighting = weightTfIdf, global=c(0.8,Inf)))
#Get the top 50:
#Start by turning it into a data frame
dtm <- tidy(dtm)
#calculate the scores
dtm_tfidf <- bind_tf_idf(dtm, term = term, document = document, n = count)
top50_terms <- dtm_tfidf %>%
group_by(term) %>%
filter(tf_idf == max(tf_idf)) %>%
ungroup() %>%
slice_max(tf_idf, n=50)
head(top50_terms)
#Now for the corpus
#Create the corpus
Corpus <- VCorpus(VectorSource(tweets_only$text))
#check that it is working
writeLines(head(strwrap(Corpus[[420]]),10))
#Now to clean the tweets; I'm splitting it for processing power, but keeping some together for efficiency
#first I'll remove all URLs
removepattern <- content_transformer(function(x,pattern){
return(gsub(pattern,"",x))
})
Corpus <- tm_map(Corpus, removepattern, "?(f|ht)(tp)(s?)(://)(.*)(.|/])(.*)")
#Next, I will make transform everything to lower case, remove whitespace, and stopwords
Corpus <- Corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(stripWhitespace) %>%
tm_map(removeWords, stopwords("english"))
#Next, I will make transform everything to lower case, remove whitespace, and stopwords
Corpus <- Corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(stripWhitespace) %>%
tm_map(removeWords, stopwords("english"))
# Change eval=FALSE in the code block. Install packages as appropriate.
library(tidyverse)
#install.packages('tm')
library(tm)
#install.packages('lubridate')
library(lubridate)
#install.packages('wordcloud')
library(wordcloud)
library(knitr)
library(broom)
library(tidytext)
trump_tweets_url <- 'https://politicaldatascience.com/PDS/Datasets/trump_tweets.csv'
tweets <- read_csv(trump_tweets_url)
#library a package necessary for date conversion
library(anytime)
#split the data using mutate and str_split
created_split <- tweets %>% mutate(date_created = str_split(created_at, pattern = " "))
#convert it to the correct col/row orientation
time <- t(as.data.frame(created_split$date_created))
#name columns
colnames(time) <- c("date","time")
#combine into our original dataset
tweets <- cbind(tweets,time)
#convert to date format for finding the oldest and newest
tweets$date <- anydate(tweets$date)
#oldest tweet day
min(tweets$date)
#newest tweet day
max(tweets$date)
#So the range of dates is 1/1/2014 to 2/14/2020
#Now to subset only to tweets
tweets_only <- tweets %>% filter(is_retweet == FALSE)
#verify our result
table(tweets_only$is_retweet)
#get the most popular
most_liked <- tweets_only %>%
slice_max(favorite_count, n=5)
most_liked$text
#Get the most retweeted
most_retweeted <- tweets_only %>%
slice_max(retweet_count, n=5)
most_retweeted$text
#Now for the corpus
#Create the corpus
Corpus <- VCorpus(VectorSource(tweets_only$text))
#check that it is working
writeLines(head(strwrap(Corpus[[420]]),10))
#Now to clean the tweets; I'm splitting it for processing power, but keeping some together for efficiency
#first I'll remove all URLs
removepattern <- content_transformer(function(x,pattern){
return(gsub(pattern,"",x))
})
Corpus <- tm_map(Corpus, removepattern, "?(f|ht)(tp)(s?)(://)(.*)(.|/])(.*)")
#Next, I will make transform everything to lower case, remove whitespace, and stopwords
Corpus <- Corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(stripWhitespace) %>%
tm_map(removeWords, stopwords("english"))
#next I'll remove punctuation and numbers
Corpus <- Corpus %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers)
#Test
writeLines(head(strwrap(Corpus[[420]]),10))
#Make the word cloud with random color and order
wc <- wordcloud(Corpus, min.freq = 3,
random.order = T,
random.color = T,
max.words = 50,
colors = sample(colors()))
#Document Term matrix
dtm <- DocumentTermMatrix(Corpus, control = list(weighting = weightTfIdf, global=c(0.8,Inf)))
#Get the top 50:
#Start by turning it into a data frame
dtm <- tidy(dtm)
#calculate the scores
dtm_tfidf <- bind_tf_idf(dtm, term = term, document = document, n = count)
top50_terms <- dtm_tfidf %>%
group_by(term) %>%
filter(tf_idf == max(tf_idf)) %>%
ungroup() %>%
slice_max(tf_idf, n=50)
head(top50_terms)
#Make the word cloud with random color and order
wc = wordcloud(Corpus, min.freq = 3,
random.order = T,
random.color = T,
max.words = 50,
colors = sample(colors()))
wc
wc.to_file("tweetwords.png")
wordcloud.to_file("tweetwords.png")
#Make the word cloud with random color and order
set.seed(125)
wc = wordcloud(Corpus, min.freq = 3,
random.order = T,
random.color = T,
max.words = 50,
colors = sample(colors()))
wc <- wordcloud(Corpus, min.freq = 3,
random.order = T,
random.color = T,
max.words = 50,
colors = sample(colors()))
#Make the word cloud with random color and order
set.seed(125)
wc <- wordcloud(Corpus, min.freq = 3,
random.order = T,
random.color = T,
max.words = 50,
colors = sample(colors()))
